---
title: 'INFX 573: Problem Set 6 - Regression'
author: "Shuyang Wu"
date: 'Due: Tuesday, November 15, 2016'
output: pdf_document
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: 

##### Instructions: #####

Before beginning this assignment, please ensure you have access to R and RStudio. 

1. Download the `problemset6.Rmd` file from Canvas. Open `problemset6.Rmd` in RStudio and supply your solutions to the assignment by editing `problemset6.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. 

3. Be sure to include well-documented (e.g. commented) code chucks, figures and clearly written text chunk explanations as necessary. Any figures should be clearly labeled and appropriately referenced within the text. 

4. Collaboration on problem sets is acceptable, and even encouraged, but each student must turn in an individual write-up in his or her own words and his or her own work. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.

5. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the R Markdown file to `YourLastName_YourFirstName_ps6.Rmd`, knit a PDF and submit the PDF file on Canvas.

##### Setup: #####

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE}
# Load standard libraries
library(tidyverse)
library(MASS)  # Modern applied statistics functions
#look at dataset
str(Boston)
names(Boston)
summary(Boston)
#Tidy the dataset
Boston$chas <- as.factor(Boston$chas)
Boston$rad <- as.integer(Boston$rad)
```

\textbf{Housing Values in Suburbs of Boston}

In this problem we will use the Boston dataset that is available in the \texttt{MASS} package. This dataset contains information about median house value for 506 neighborhoods in Boston, MA. Load this data and use it to answer the following questions.

\benum

\item Describe the data and variables that are part of the \texttt{Boston} dataset. Tidy data as necessary.

The Boston dataset contains per capita crime rate by town, proportion of residential land zoned for lots over 25,000 sq.ft., proportion of non-retail business acres per town, Charles River dummy variable (= 1 if tract bounds river; 0 otherwise), nitrogen oxides concentration (parts per 10 million), average number of rooms per dwelling, proportion of owner-occupied units built prior to 1940, weighted mean of distances to five Boston employment centres, index of accessibility to radial highways, full-value property-tax rate per ten thousand, pupil-teacher ratio by town, proportion of blacks by town, lower status of the population (percent), median value of owner-occupied homes in $1000s.

\item Consider this data in context, what is the response variable of interest? Discuss how you think some of the possible predictor variables might be associated with this response.

Response variable is the median value of owner-occupided homes. Intuitively, I think number of rooms, distances to five employment centres, index of accessibility to radial highways could be associated with this response.

\item For each predictor, fit a simple linear regression model to predict the response. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

```{r, message=FALSE}
#simple linear regressions
fit.crim <- lm(medv ~ crim, data = Boston)
summary(fit.crim)
fit.zn <- lm(medv ~ zn, data = Boston)
summary(fit.zn) #significant
fit.indus <- lm(medv ~ indus, data = Boston)
summary(fit.indus) #significant
fit.nox <- lm(medv ~ nox, data = Boston)
summary(fit.nox) #significant
fit.age <- lm(medv ~ age, data = Boston)
summary(fit.age) #significant
fit.tax <- lm(medv ~ tax, data = Boston)
summary(fit.tax) #significant
fit.ptratio <- lm(medv ~ ptratio, data = Boston)
summary(fit.ptratio) #significant
fit.black <- lm(medv ~ black, data = Boston)
summary(fit.black) #significant
fit.lstat <- lm(medv ~ lstat, data = Boston)
summary(fit.lstat) #significant
fit.chas <- lm(medv ~ chas, data = Boston)
summary(fit.chas)

#number of rooms
fit.rm <- lm(medv ~ rm, data = Boston)
summary(fit.rm)
plot(Boston$medv, main = "linear regression plot of rm vs medv")
lines(Boston$rm,predict(fit.rm))

#distances to five employment centres
fit.dis <- lm(medv ~ dis, data = Boston)
summary(fit.dis)
plot(Boston$medv, main = "linear regression plot of dis vs medv")
lines(Boston$dis,predict(fit.dis))

#index of accessibility to radial highways
fit.rad <- lm(medv ~ rad, data = Boston)
summary(fit.rad)
plot(Boston$medv, main = "linear regression plot of rad vs medv")
lines(Boston$rad,predict(fit.rad))
```
All of these three models show statistically significant associations between the predictor and the response (p << 0.05). But the plots show otherwise.

\item Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0: \beta_j = 0$?

```{r, message=FALSE}
#multiple linear regressions
fit.all <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = Boston)
summary(fit.all)
plot(Boston$medv)
lines(Boston$rm,predict(fit.all))
```
For predictors zn, chas1, nox, rm, dis, rad, ptratio, black and lstat (Proportion of residential land zoned for lots over 25,000 sq.ft, Charles River dummy variable, nitrogen oxides concentration, average number of rooms per dwelling, weighted mean of distances to five Boston employment centres, index of accessibility to radial highways, pupil-teacher ratio, proportion of blacks and lower status of the population(percent) ), we can reject the null hypothesis because their corresponding p values are smaller than 0.05. 

\item How do your results from (3) compare to your results from (4)? Create a plot displaying the univariate regression coefficients from (3) on the x-axis and the multiple regression coefficients from part (4) on the y-axis. Use this visualization to support your response.

```{r, message=FALSE}
#get simple linear regressions' coefficients

crim <- fit.crim$coefficients
comp <- as.data.frame(crim)
comp$zn <- fit.zn$coefficients
comp$indus <- fit.indus$coefficients
comp$chas <- fit.chas$coefficients
comp$nox <- fit.nox$coefficients
comp$rm <- fit.rm$coefficients
comp$age <- fit.age$coefficients
comp$dis <- fit.dis$coefficients
comp$rad <- fit.rad$coefficients
comp$tax<- fit.tax$coefficients
comp$ptratio <- fit.ptratio$coefficients
comp$black <- fit.black$coefficients
comp$lstat <- fit.lstat$coefficients

#multiple regression
coe <- fit.all$coefficients[c(-1)]
plot(as.numeric(comp[2,]),as.numeric(coe), xlab = "coefficients from linear regressions", ylab = "coefficients from multiple linear regression")
```

Based on the plot, oefficient for all variables from univariate linear regressions is similar to their corresponding coefficient from multiple linear regression, which means [4] supports [3].

\item Is there evidence of a non-linear association between any of the predictors and the response? To answer this question, for each predictor $X$ fit a model of the form:

$$ Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon $$
```{r, message=FALSE}
#non-linear regressions' coefficients
predictors <- names(Boston[,-ncol(Boston)])
r2 <- NULL
for(i in predictors){
    tmp <- lm(Boston$medv ~ Boston[,i] + Boston[,i]^2 + Boston[,i]^3)
    r2[i] <- summary(tmp)$r.squared
}
r2
```
There is evidence of a cubic polynomial association for rm and lstat because the R-square values are significantly higher, other predictors does not seem to have such association.

\item Consider performing a stepwise model selection procedure to determine the bets fit model. Discuss your results. How is this model different from the model in (4)?

```{r, message=FALSE}
#model selection 
step <- stepAIC(fit.all, direction="both")
step$anova # display results
```

The predictors suggested by stepwise model selection are completely different from the ones suggested by multiple linear regression. (Stepwise model selection suggests: age, indus, whereas multiple linear regression suggests: zn, chas1, nox, rm, dis, rad, ptratio, black and lstat. No overlap.)

\item Evaluate the statistical assumptions in your regression analysis from (7) by performing a basic analysis of model residuals and any unusual observations. Discuss any concerns you have about your model.

Stepwise Linear Regression assumption:
Linear relationship, multivariate normality, no or little multicollinearity, no auto-correlation, homoscedasticity. Because the Residual vs Fitted plot shows a non-linear relationship, it means that our model has some non-linearity, unequal error variances, and outliers.

```{r, message=FALSE}
#model residuals
plot(step)
```

\eenum
